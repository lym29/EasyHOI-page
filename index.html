<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EasyHOI</title>
  <link rel="icon" type="image/x-icon" href="static/images/easy-icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lym29.github.io/" target="_blank"> Yumeng Liu</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.xxlong.site/" target="_blank"> Xiaoxiao Long</a>  <sup>3, <a href="mailto:xxlong@connect.hku.hk"><i class="far fa-envelope"></i></a> </sup>,
              </span>
              <span class="author-block">
                <a href="https://yizhifengyeyzm.github.io/" target="_blank"> Zemin Yang</a>  <sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://liuyuan-pal.github.io/" target="_blank"> Yuan Liu </a> <sup>3,4</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.mpi-inf.mpg.de/~mhaberma/" target="_blank"> Marc Habermann</a> <sup>5</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank">Christian Theobalt</a> <sup>5</sup>,
              </span>
              <span class="author-block">
                <a href="https://yuexinma.me/"> Yuexin Ma</a><sup>2, <a href="mailto:mayuexin@shanghaitech.edu.cn.com"><i class="far fa-envelope"></i></a> </sup>,
              </span>
              <span class="author-block">
                <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html"> Wenping Wang</a><sup>6</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>The University of Hong Kong,
                <sup>2</sup>ShanghaiTech University,
                <sup>3</sup>Hong Kong University of Science and Technology,
                <sup>4</sup>Nanyang Technological University,
                <sup>5</sup>Max Planck Institute for Informatics,
                <sup>6</sup>Texas A&M University<br>
              </span>
            </div>

            <div class="is-size-8 publication-authors">
                <br>
                <i class="far fa-envelope"></i> Corresponding Authors
                <br>
                This paper was finalized during the first author's visit to ShanghaiTech University.
              </span>
            </div>

            <div style="margin-top: 20px; padding: 15px; background-color: #f9f9f9; border: 1px solid #e0e0e0; border-radius: 8px; font-family: 'Google Sans', Arial, sans-serif; text-align: center;">
              <p style="font-size: 18px; color: #333; margin: 0;">
                  ðŸŽ‰ <strong>EasyHOI has been accepted at CVPR 2025!</strong> <br>
                  We look forward to meeting you in <strong>Nashville</strong>!
              </p>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                    <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2411.14280" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/arxiv-logomark.svg" alt="arXiv" style="width: 20px; height: auto;" />
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/Appendix.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/lym29/EasyHOI" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Dataset link -->
                <!-- <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1d5OSVuvYr6-OMG_cuKeoFy6lNdezX8nc?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google-drive"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.13853" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> 
-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Our work aims to reconstruct hand-object interactions from a single-view image, which is a fundamental but ill-posed task. Unlike methods that reconstruct from videos, multi-view images, or predefined 3D templates, single-view reconstruction faces significant challenges due to inherent ambiguities and occlusions. These challenges are further amplified by the diverse nature of hand poses and the vast variety of object shapes and sizes. Our key insight is that current foundational models for segmentation, inpainting, and 3D reconstruction robustly generalize to in-the-wild images, which could provide strong visual and geometric priors for reconstructing hand-object interactions. Specifically, given a single image, we first design a novel pipeline to estimate the underlying hand pose and object shape using off-the-shelf large models. Furthermore, with the initial reconstruction, we employ a prior-guided optimization scheme, which optimizes hand pose to comply with 3D physical constraints and the 2D input image content. We perform experiments across several datasets and show that our method consistently outperforms baselines and faithfully reconstructs a diverse set of hand-object interactions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!--Your image here -->
        <img src="static/images/teaser.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          We present a novel approach for single-view hand-object interaction reconstruction that unleashes the power of foundational models to address inherent ambiguities and occlusions, demonstrating superior performance across diverse datasets.
        </h2>
      </div>
      
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/pipeline.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The illustration of our pipeline.
        </h2>
      </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->









<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">More Results</h2>
      <div class="gif-container">
        <img src="static/images/test.gif" alt="More Results" class="custom-gif" />
      </div>
    </div>
  </div>
</section>


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ruTAuu_ltaE?si=7X3qaXUc5DBijoVU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->



<!-- Video carousel -->
<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> 
-->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{liu2024easyhoi,
          title={EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild},
          author={Liu, Yumeng and Long, Xiaoxiao and Yang, Zemin and Liu, Yuan and Habermann, Marc and Theobalt, Christian and Ma, Yuexin and Wang, Wenping},
          journal={arXiv preprint arXiv:2411.14280},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p><strong>Acknowledgement:</strong> 
            This work was supported by NSFC (No.62206173), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (KLIP-HuMaCo).

            The authors thank Qi Jiang and Yaxun Yang (ShanghaiTech University) for their helpful discussions on this project.
          </p>

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
